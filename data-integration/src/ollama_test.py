import os
import argparse
from tqdm import tqdm
from utils import readFile, writeToFile
import ollama

# import asyncio


def main():
    parser = argparse.ArgumentParser(
        description="""Prompt a local Ollama service to analyze a text file"""
    )
    parser.add_argument("input", help="Specify the input text file")
    parser.add_argument("output", help="Specify the output text file")
    parser.add_argument("prompt", help="Specify the prompt")
    parser.add_argument(
        "--separator",
        default=" : ",
        help="""Specify the separator between the prompt and the text file""",
    )
    parser.add_argument(
        "-f",
        "--format",
        default="",
        help="""Specify the ollama response format.""",
    )
    parser.add_argument(
        "-m",
        "--model",
        default="mistral:7b",
        help="""Specify the ollama model tag. If a modelfile is given, this
            parameter is used as the model name.""",
    )
    parser.add_argument(
        "-s",
        "--stream",
        action="store_true",
    )
    parser.add_argument(
        "--modelfile",
        default=None,
        help="""Specify the ollama modelfile filepath.""",
    )
    parser.add_argument(
        "-u",
        "--url",
        default=None,
        help="""Specify an RESTfull Ollama api URL.""",
    )
    parser.add_argument(
        "-t",
        "--token",
        default=None,
        help="""Specify a client authorization (bearer) token.""",
    )

    args = parser.parse_args()

    input_path = os.path.normpath(args.input)
    output_path = os.path.normpath(args.output)
    client = None

    if args.url is not None and args.token is not None:
        print("using client")
        headers = {
            "Content-Type": "application/json",
            "accept": "application/json",
            "Authorization": f"Bearer {args.token}",
        }
        # client = ollama.AsyncClient(host=args.url, headers=headers)
        client = ollama.Client(host=args.url, headers=headers)

    print(f"reading {input_path}")
    text = readFile(input_path)
    print("sending prompt")
    prompt = args.prompt + args.separator + text
    response = ""

    response = sendPrompt(
        model=args.model,
        prompt=prompt,
        modelfile_path=args.modelfile,
        format=args.format,
        client=client if client is not None else ollama,
    )

    print(f"writing response to {output_path}")
    if args.stream:
        if os.path.exists(output_path):
            os.remove(output_path)
        with open(output_path, "a") as file:
            for chunk in response:
                file.write(chunk.response)
    else:
        writeToFile(output_path, response["response"])  # type: ignore


def sendPrompt(model: str, prompt: str, modelfile_path=None, format="", client=ollama):
    """
    Generate a text response using the local Ollama service based on a given model and
    prompt, handling errors and retries if necessary.

    :param model: Specify the model that will be used for generating the response. Should
        be a string that identifies the specific model to be used by Ollama
    :param prompt: A string that represents the input text for the generated response
    :param modelfile_path: Specify the path to a file that contains the model data.
    :param format: Specify the format in which you want the response to be generated by
        the Ollama service.

    return: The response generated by the Ollama service.
    """
    response = {}
    try:
        response = client.generate(
            model=model, prompt=prompt, format=format  # type: ignore
        )
    except ollama.ResponseError as e:
        if e.status_code == 404:
            print(f"error code received: {e.status_code}")
            if modelfile_path is None:
                print(f"pulling {model}")
                client.pull(model)

                print("resending prompt")
                response = client.generate(
                    model=model, prompt=prompt, format=format  # type: ignore
                )
            else:
                print(f"creating {model} based on {modelfile_path}")
                modelfile = readFile(modelfile_path)
                for server_response in client.create(model=model, modelfile=modelfile):
                    print(server_response)

                print("resending prompt")
                response = client.generate(
                    model=model, prompt=prompt, format=format  # type: ignore
                )
        else:
            raise e

    return response


def pullShowProgress(model: str, stream=True):
    """
    Show the progress of an Ollama model pull in the console. Code adapted from
    https://github.com/ollama/ollama-python/blob/main/examples/pull-progress
    """
    current_digest, bars = "", {}
    for progress in ollama.pull(model, stream):
        digest = progress.get("digest", "")  # type: ignore
        if digest != current_digest and current_digest in bars:
            bars[current_digest].close()

        if not digest:
            print(progress.get("status"))  # type: ignore
            continue

        if digest not in bars and (total := progress.get("total")):  # type: ignore
            bars[digest] = tqdm(
                total=total,
                desc=f"pulling {digest[7:19]}",
                unit="B",
                unit_scale=True,
            )

        if completed := progress.get("completed"):  # type: ignore
            bars[digest].update(completed - bars[digest].n)

        current_digest = digest


if __name__ == "__main__":
    main()
